{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "phantom-strand",
   "metadata": {},
   "source": [
    "# Abstractive Text Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-belfast",
   "metadata": {},
   "source": [
    "## 1. Downloading and Processing the data\n",
    "\n",
    "\n",
    "![alt text](data_processing.png \"Pre-Process Flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-feature",
   "metadata": {},
   "source": [
    "## 2. K-Means Clustering of data points after BERT tokenizing \n",
    "\n",
    "\n",
    "* Numerical representation of sentences is obtained through the BERT model, which generaly is representation of array of arrays. These feature vector representing text can be clustered using the K-means algorithm.\n",
    "\n",
    "* An important metric in clustering is the Within Cluster the Sum of Squares (WCSS). This metric is measured by the squared average distance of all the points within a cluster to the cluster centroid. To find the average WCSS, calculate the average across all the clusters. The metric WCSS is mainly used to measure the irregularity of the observations within each cluster\n",
    "\n",
    "* Between Clusters Sum of Squares (BCSS) is the measure of the squared average distance between all cluster centroids. This is computed by the eucledean distance between other cluster centroid, which is iterated over all the clusters.  The metric BCSS mainly measures the variation between all clusters, father away the cluster higher the variation comparetively.\n",
    "\n",
    "![alt text](kmeans_clustering.png \"K-Means of Tokenized texts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-night",
   "metadata": {},
   "source": [
    "> ## Uses:-\n",
    "    > As its know WCSS(Within Cluster the Sum of Squares) gives the variability within the same cluster, so higher sentences can be choosen with higher weightage as they contain more context. The reverse applies for sentences with lower WCSS\n",
    "    > We can also determine the Appropriate number of summary length needed based on the WCSS for retaining maximum context. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-manner",
   "metadata": {},
   "source": [
    "## 3. Bi-Directional Attention Encoder-Decoder Model\n",
    "\n",
    "> ### Why not only Attention based LSTM ?\n",
    "    > LSTM, GRU models that are based on RNN variants have good performance when generative abstractive summary, with attention mechanism in place, but issue occurs over long sequence of texts\n",
    "    >\n",
    "    > **There are 3 parts to the entire model: The Encoder, The Decoder, The Attention Layer.**\n",
    "    > - **The Encoder** - This a LSTM model that takes in the full text of the review to learn how to transform the context of those reviews into neural network parameters.\n",
    "    > - **The Decoder** - This a LSTM model that takes in the learned neural network parameters from `The Encoder` and the 1st to 2nd last word of the text summary data to learn the patterns of the summary given w.r.t. the full text review.\n",
    "    > - **The Attention Layer** - Adjusts the attention of The Decoder based on the contextual understanding of both the full text review and the full text summary. The intuition behind the Attention Layer is basically finding and focusing on the essence of the question or text. For example, if the question was, \"What animal do you like?\", simply focusing on the word 'animal' would get you to consider all animals for this context. Thereafter, focusing on the word 'like' would get you to answer with your favorite animal straightaway. Hence, instead of fully considering all 5 words, by focusing on less than half the question and 'blurring' the rest, we are able to generate a response already.\n",
    "\n",
    "![alt text](enc_dec_attention.png \"Bi-Directional Encoder Decoder Attention Model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-suspect",
   "metadata": {},
   "source": [
    "## 4. Training\n",
    "\n",
    "> Currently the current NLP trends aim for using pre-trained Transformers using self-supervised objectives on large text data and then use supervised fine-tuning on a smaller dataset, and for this we can use the readily available model that are trained on million of para meters.\n",
    "> Fine Tuning can be achived by `distillation` methods where existing parameters are taken into consideration and trained on required dataset to further distill and fine tune.\n",
    "> SFT (Shrink and Fine Tune) can be used for tuning, where the basic idea is to shrink a fine-tuned teacher/original model to smaller size and re-fine-tune this new student/derived model. The student/derived model is intialized by copying arbitary parameters from teacher/original. \n",
    "    > * The student model can have lower decoder layer from teacher. As decoder are the learning units during distillation we need not reduce encoders.\n",
    "    > * The student is trained on new data with objective to minimize the loss function (e.g. Cross Entropy Loss)\n",
    "    > * The Length of target sequence can be provided as estimated by the clustering method mentioned above\n",
    "\n",
    "![alt text](transfer_learning.png \"Transfer Learning\") ![alt text](sft_distillation.png \"Shrink and Fine Tuning Distillation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-binary",
   "metadata": {},
   "source": [
    "## 5. Testing and Evaluation\n",
    "\n",
    "> * ROUGE metrics measure the n-gram overlap between a reference summary (usually generated manually or given as golden sumaary of text), and a system generated one. This nature of ROUGE metrics makes it possible for the generative models that optimizes for this metric can compromise on the quality and the readability of the generated summary\n",
    "> * BLEU metrics is related to precision, it also compares n-grams of a generated summary with the n-grams of a reference/golden summary and count the number of matches. For producing a score the generated summary segment/sentences are combined using Geometric Mean with added brevity penalty (prevents smaller sentences to have very high scores)\n",
    "> * ROUGE and BLUE generally differ from each other as ROUGE is a recall based method and BLEU being a precision metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-withdrawal",
   "metadata": {},
   "source": [
    "## 6. Further Improvement\n",
    "> * Can be trained on multiple dataset available having a reference/golden summary of varying lengths\n",
    "> * Hugely diffrent domain dataset or text data can be used to further increase understanding of model\n",
    "> * Sentence Gap or word masks can be implemented to reduce overfitting\n",
    "> * Online training can be achived if that can be made manual using QnA model, where user are given ability to rectify generated summary data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-cruise",
   "metadata": {},
   "source": [
    "*Motivation on the Summarization from the below papers*\n",
    "---\n",
    "> * `Extractive Text Summarization using Dynamic Clustering and Co-Reference on BERT` by Srikanth, A., Umasankar, A. S., Thanu, S., & Nirmala, S. J.\n",
    ">\n",
    "> * `Bidirectional Attentional Encoder-Decoder Model and Bidirectional Beam Search for Abstractive Summarization` by Kamal Al-Sabahi, Zhang Zuping, Yang Kang\n",
    ">\n",
    "> * `PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization` by Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter J. Liu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
