{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "phantom-strand",
   "metadata": {},
   "source": [
    "# Abstractive Text Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-belfast",
   "metadata": {},
   "source": [
    "## 1. Downloading and Processing the data\n",
    "\n",
    "\n",
    "![alt text](data_processing.png \"Pre-Process Flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-feature",
   "metadata": {},
   "source": [
    "## 2. K-Means Clustering of data points after BERT tokenizing \n",
    "\n",
    "\n",
    "* Numerical representation of sentences is obtained through the BERT model, which generaly is representation of array of arrays. These feature vector representing text can be clustered using the K-means algorithm.\n",
    "\n",
    "* An important metric in clustering is the Within Cluster the Sum of Squares (WCSS). This metric is measured by the squared average distance of all the points within a cluster to the cluster centroid. To find the average WCSS, calculate the average across all the clusters. The metric WCSS is mainly used to measure the irregularity of the observations within each cluster\n",
    "\n",
    "* Between Clusters Sum of Squares (BCSS) is the measure of the squared average distance between all cluster centroids. This is computed by the eucledean distance between other cluster centroid, which is iterated over all the clusters.  The metric BCSS mainly measures the variation between all clusters, father away the cluster higher the variation comparetively.\n",
    "\n",
    "![alt text](kmeans_clustering.png \"K-Means of Tokenized texts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-night",
   "metadata": {},
   "source": [
    "> ## Uses:-\n",
    "    > As its know WCSS(Within Cluster the Sum of Squares) gives the variability within the same cluster, so higher sentences can be choosen with higher weightage as they contain more context. The reverse applies for sentences with lower WCSS\n",
    "    > We can also determine the Appropriate number of summary length needed based on the WCSS for retaining maximum context. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-manner",
   "metadata": {},
   "source": [
    "## 3. Bi-Directional Attention Encoder-Decoder Model\n",
    "\n",
    "> ### Why not only Attention based LSTM ?\n",
    "    > LSTM, GRU models that are based on RNN variants have good performance when generative abstractive summary, with attention mechanism in place, but issue occurs over long sequence of texts\n",
    "    >\n",
    "    > **There are 3 parts to the entire model: The Encoder, The Decoder, The Attention Layer.**\n",
    "    > - **The Encoder** - This a LSTM model that takes in the full text of the review to learn how to transform the context of those reviews into neural network parameters.\n",
    "    > - **The Decoder** - This a LSTM model that takes in the learned neural network parameters from `The Encoder` and the 1st to 2nd last word of the text summary data to learn the patterns of the summary given w.r.t. the full text review.\n",
    "    > - **The Attention Layer** - Adjusts the attention of The Decoder based on the contextual understanding of both the full text review and the full text summary. The intuition behind the Attention Layer is basically finding and focusing on the essence of the question or text. For example, if the question was, \"What animal do you like?\", simply focusing on the word 'animal' would get you to consider all animals for this context. Thereafter, focusing on the word 'like' would get you to answer with your favorite animal straightaway. Hence, instead of fully considering all 5 words, by focusing on less than half the question and 'blurring' the rest, we are able to generate a response already.\n",
    "\n",
    "![alt text](enc_dec_attention.png \"Bi-Directional Encoder Decoder Attention Model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
